{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dataset = tf.data.Dataset.range(100)\n",
    "\n",
    "# split into windows and flatten\n",
    "WINDOW_SIZE = 10\n",
    "dataset = dataset.window(WINDOW_SIZE+1, shift=1, drop_remainder=True)\n",
    "dataset_flat = dataset.flat_map(lambda window: window.batch(WINDOW_SIZE+1))\n",
    "\n",
    "# split into x and y\n",
    "dataset_flat = dataset_flat.map(lambda window: (window[:-1], window[-1:]))\n",
    "\n",
    "# our data has 1st order array dimensionality, a.k.a. not specifically defined\n",
    "# shape=(10,)\n",
    "next(iter(dataset_flat))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(5, 2), dtype=int64, numpy=\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]], dtype=int64)>"
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "# we can use tf.reshape() or tf.expand_dims() to change tensor shapes\n",
    "tf.reshape(next(iter(dataset_flat))[0], [-1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(10, 1), dtype=int64, numpy=\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6],\n       [7],\n       [8],\n       [9]], dtype=int64)>"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "# in case of univariate data (feature dimensionality == 1) we can use expand_dims\n",
    "# to create 3d data suitable for tf.keras.layers.LSTM()\n",
    "tf.expand_dims(next(iter(dataset_flat))[0], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and apply it in a systematic way using tf.keras.layers.Lambda()\n",
    "# by building into a model itself\n",
    "SEQ_LENGTH = next(iter(dataset_flat))[0].shape[0]\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[SEQ_LENGTH]),\n",
    "    tf.keras.layers.LSTM(8),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_25\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlambda_25 (Lambda)           (None, 10, 1)             0         \n_________________________________________________________________\nlstm_20 (LSTM)               (None, 8)                 320       \n_________________________________________________________________\ndense_24 (Dense)             (None, 1)                 9         \n=================================================================\nTotal params: 329\nTrainable params: 329\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = tf.reshape(next(iter(dataset_flat))[0], [5, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_31\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlambda_31 (Lambda)           (None, 5, 2)              0         \n_________________________________________________________________\nlstm_26 (LSTM)               (None, 8)                 352       \n_________________________________________________________________\ndense_30 (Dense)             (None, 1)                 9         \n=================================================================\nTotal params: 361\nTrainable params: 361\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# take one example and get feature dimensionality\n",
    "SEQ_LENGTH = sample.shape[0]\n",
    "FEATURE_DIMENSIONALITY = sample.shape[1]\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Lambda(lambda x: tf.reshape(x, [-1, SEQ_LENGTH, FEATURE_DIMENSIONALITY]),\\\n",
    "                           input_shape=[None]),\n",
    "    tf.keras.layers.LSTM(8, input_shape=[SEQ_LENGTH, FEATURE_DIMENSIONALITY]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate data and not known time sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm (LSTM)                  (None, 32)                4480      \n_________________________________________________________________\ndense (Dense)                (None, 1)                 33        \n=================================================================\nTotal params: 4,513\nTrainable params: 4,513\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# If you don't know the size of dataset's time dimension,\n",
    "# just use None at 'input_shape' parameter\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(32, input_shape=[None, 2]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you don't know size of timestep - you create correct shape just before Dataset construction. At that moment you should know at least feature dimensionality and amount of separate samples \n",
    "\n",
    "Below is example where data is range from 0 to 99 and we assume that:\n",
    "- even number come from one user\n",
    "- odd number from second\n",
    "- each example represented by 2 features (not crucial this time how digits are paired)\n",
    "\n",
    ".. so we prepare dataset with correct shape to feed into LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(2, 25, 2)\n[[ 1  3]\n [ 5  7]\n [ 9 11]]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Difference between array.reshape() and tf.reshape() is that\n",
    "# numpy version returns you numpy array\n",
    "# whereas tf version returns you tensor\n",
    "\n",
    "# one user data\n",
    "SAMPLES = 2\n",
    "FEATURES = 2\n",
    "DATA = range(100)\n",
    "\n",
    "user1 = []\n",
    "user2 = []\n",
    "\n",
    "for i in DATA:\n",
    "    if i % 2:\n",
    "        user1.append(i)\n",
    "    else:\n",
    "        user2.append(i)\n",
    "\n",
    "\n",
    "correct_shape_array = np.array(user1+user2).reshape([SAMPLES,-1,FEATURES])\n",
    "dumb_labels = np.array([0,1]).reshape(2,1)\n",
    "\n",
    "print(correct_shape_array.shape)\n",
    "print(correct_shape_array[0,0:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 2 samples\n2/2 [==============================] - 2s 1s/sample - loss: 0.7356 - mae: 0.7356\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x14ddc899a08>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "model.fit(correct_shape_array, dumb_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to use `tf.data.Dataset`, specify batch size (you may shuffle as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<tf.Tensor: shape=(25, 2), dtype=int32, numpy=\n array([[ 1,  3],\n        [ 5,  7],\n        [ 9, 11],\n        [13, 15],\n        [17, 19],\n        [21, 23],\n        [25, 27],\n        [29, 31],\n        [33, 35],\n        [37, 39],\n        [41, 43],\n        [45, 47],\n        [49, 51],\n        [53, 55],\n        [57, 59],\n        [61, 63],\n        [65, 67],\n        [69, 71],\n        [73, 75],\n        [77, 79],\n        [81, 83],\n        [85, 87],\n        [89, 91],\n        [93, 95],\n        [97, 99]])>, <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# prepare tf.data.Dataset\n",
    "multivariate_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                            (correct_shape_array, dumb_labels))\n",
    "\n",
    "# in the end tf.data.Dataset has same 3D structure\n",
    "# shape=(25,2) because we take only 1 example and its dimension is omitted\n",
    "next(iter(multivariate_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 2 steps\n2/2 [==============================] - 0s 35ms/step - loss: 0.5224 - mae: 0.5224\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x14de0c3ac48>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "model.fit(multivariate_dataset.shuffle(10).batch(1).prefetch(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note about `.prefetch(1)`\n",
    "\n",
    "From https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch:\n",
    "```\n",
    "Most dataset input pipelines should end with a call to prefetch.  \n",
    "This allows later elements to be prepared while the current element  \n",
    "is being processed. This often improves latency and throughput,  \n",
    "at the cost of using additional memory to store prefetched elements.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bita866c8c9cf084ebd97979ec2adc3c734",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}