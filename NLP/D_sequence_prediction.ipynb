{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fragment from Zobel's \"Writing for Computer Science\"\n",
    "# Will be used for training\n",
    "raw_corpus = \"\"\"Some advisors, for example, set their students problems such as verifying a proof\n",
    "in a published paper and seeing whether it can be applied to variants of the theorem,\n",
    "thus, in effect, getting the student to explore the limits at which the theorem no longer\n",
    "applies. Another example is to attempt to confirm someone elseâ€™s results, by downloading \n",
    "code or by developing a fresh implementation. The difficulties encountered\n",
    "in such efforts are a fertile source of research questions. Other advisors immediately\n",
    "start their students on activities that are expected to lead to a research publication. It\n",
    "is in this last case that the model of advising as apprenticeship is most evident.\n",
    "Typically, in the early stages the advisor specifies each small step the student\n",
    "should take: running a certain experiment, identifying a suitable source of data,\n",
    "searching the literature to resolve a particular question, or writing one small section\n",
    "of a proposed paper. As students mature into researchers, they become more independent, \n",
    "often by anticipating what their advisors will ask, while advisors gradually leave\n",
    "more space for their students to assert this independence. Over time, the relationship\n",
    "becomes one of guidance rather than management\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = raw_corpus.lower().split('\\n')\n",
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit_on_texts(corpus)\n",
    "word_index = t.word_index\n",
    "vocab_size = len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we create sequences not from whole corpus at once, but modifying line by line. This is made in order to add not only full version of line, but cropped versions, in order to train RNN predict next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "\n",
    "for line in corpus:\n",
    "    line_seq = t.texts_to_sequences([line])[0]\n",
    "    # starting with 1 as we want minimum 2 elements in sequence\n",
    "    for idx in range(1,len(line_seq)):\n",
    "        n_seq = line_seq[:idx+1]\n",
    "        input_sequences.append(n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[29, 7],\n [29, 7, 13],\n [29, 7, 13, 14],\n [29, 7, 13, 14, 30],\n [29, 7, 13, 14, 30, 8]]"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "input_sequences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "16"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_input_seq = np.array(pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 29,  7],\n       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 29,  7, 13],\n       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 29,  7, 13, 14],\n       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 29,  7, 13, 14, 30],\n       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 29,  7, 13, 14, 30,  8]])"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "padded_input_seq[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 29]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0 29  7]\n [ 0  0  0  0  0  0  0  0  0  0  0  0 29  7 13]\n [ 0  0  0  0  0  0  0  0  0  0  0 29  7 13 14]\n [ 0  0  0  0  0  0  0  0  0  0 29  7 13 14 30]]\n[ 7 13 14 30  8]\n"
    }
   ],
   "source": [
    "# Last element of those sequences is what we want predict (next word)\n",
    "x = padded_input_seq[:,:-1]\n",
    "labels = padded_input_seq[:,-1]\n",
    "print(x[0:5])\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encode labels\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(labels, num_classes=vocab_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[129 130]\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
    }
   ],
   "source": [
    "print(labels[-2:])\n",
    "print(y[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # vocab_size+1 as we count OOV, max_sequence_lenth-1 as last word taken for y\n",
    "    tf.keras.layers.Embedding(vocab_size+1, 64, input_length=max_sequence_length-1),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    # Last layer should classify amongst all words\n",
    "    # so Nr of units is equal with vocab size + OOV and activation softmax\n",
    "    tf.keras.layers.Dense(vocab_size+1, activation='softmax')\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 15, 64)            8384      \n_________________________________________________________________\nbidirectional (Bidirectional (None, 64)                24832     \n_________________________________________________________________\ndense (Dense)                (None, 131)               8515      \n=================================================================\nTotal params: 41,731\nTrainable params: 41,731\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/500\n179/179 [==============================] - 0s 383us/sample - loss: 0.1406 - accuracy: 0.9944\nEpoch 315/500\n179/179 [==============================] - 0s 375us/sample - loss: 0.1398 - accuracy: 0.9944\nEpoch 316/500\n179/179 [==============================] - 0s 367us/sample - loss: 0.1381 - accuracy: 0.9944\nEpoch 317/500\n179/179 [==============================] - 0s 376us/sample - loss: 0.1384 - accuracy: 0.9944\nEpoch 318/500\n179/179 [==============================] - 0s 369us/sample - loss: 0.1364 - accuracy: 0.9888\nEpoch 319/500\n179/179 [==============================] - 0s 388us/sample - loss: 0.1353 - accuracy: 0.9944\nEpoch 320/500\n179/179 [==============================] - 0s 387us/sample - loss: 0.1350 - accuracy: 0.9944\nEpoch 321/500\n179/179 [==============================] - 0s 371us/sample - loss: 0.1341 - accuracy: 0.9944\nEpoch 322/500\n179/179 [==============================] - 0s 382us/sample - loss: 0.1328 - accuracy: 0.9944\nEpoch 323/500\n179/179 [==============================] - 0s 375us/sample - loss: 0.1314 - accuracy: 0.9944\nEpoch 324/500\n179/179 [==============================] - 0s 382us/sample - loss: 0.1313 - accuracy: 0.9944\nEpoch 325/500\n179/179 [==============================] - 0s 385us/sample - loss: 0.1306 - accuracy: 0.9944\nEpoch 326/500\n179/179 [==============================] - 0s 384us/sample - loss: 0.1287 - accuracy: 0.9944\nEpoch 327/500\n179/179 [==============================] - 0s 373us/sample - loss: 0.1270 - accuracy: 0.9944\nEpoch 328/500\n179/179 [==============================] - 0s 373us/sample - loss: 0.1259 - accuracy: 0.9944\nEpoch 329/500\n179/179 [==============================] - 0s 384us/sample - loss: 0.1251 - accuracy: 0.9944\nEpoch 330/500\n179/179 [==============================] - 0s 377us/sample - loss: 0.1239 - accuracy: 0.9944\nEpoch 331/500\n179/179 [==============================] - 0s 371us/sample - loss: 0.1238 - accuracy: 0.9944\nEpoch 332/500\n179/179 [==============================] - 0s 368us/sample - loss: 0.1220 - accuracy: 0.9944\nEpoch 333/500\n179/179 [==============================] - 0s 386us/sample - loss: 0.1224 - accuracy: 0.9944\nEpoch 334/500\n179/179 [==============================] - 0s 373us/sample - loss: 0.1214 - accuracy: 0.9944\nEpoch 335/500\n179/179 [==============================] - 0s 401us/sample - loss: 0.1207 - accuracy: 0.9944\nEpoch 336/500\n179/179 [==============================] - 0s 379us/sample - loss: 0.1185 - accuracy: 0.9944\nEpoch 337/500\n179/179 [==============================] - 0s 384us/sample - loss: 0.1180 - accuracy: 0.9944\nEpoch 338/500\n179/179 [==============================] - 0s 382us/sample - loss: 0.1174 - accuracy: 0.9944\nEpoch 339/500\n179/179 [==============================] - 0s 375us/sample - loss: 0.1164 - accuracy: 0.9944\nEpoch 340/500\n179/179 [==============================] - 0s 386us/sample - loss: 0.1160 - accuracy: 0.9944\nEpoch 341/500\n179/179 [==============================] - 0s 381us/sample - loss: 0.1145 - accuracy: 0.9944\nEpoch 342/500\n179/179 [==============================] - 0s 387us/sample - loss: 0.1136 - accuracy: 0.9944\nEpoch 343/500\n179/179 [==============================] - 0s 378us/sample - loss: 0.1130 - accuracy: 0.9944\nEpoch 344/500\n179/179 [==============================] - 0s 374us/sample - loss: 0.1125 - accuracy: 0.9944\nEpoch 345/500\n179/179 [==============================] - 0s 553us/sample - loss: 0.1120 - accuracy: 0.9944\nEpoch 346/500\n179/179 [==============================] - 0s 359us/sample - loss: 0.1115 - accuracy: 0.9944\nEpoch 347/500\n179/179 [==============================] - 0s 391us/sample - loss: 0.1107 - accuracy: 0.9944\nEpoch 348/500\n179/179 [==============================] - 0s 378us/sample - loss: 0.1096 - accuracy: 0.9944\nEpoch 349/500\n179/179 [==============================] - 0s 390us/sample - loss: 0.1082 - accuracy: 0.9944\nEpoch 350/500\n179/179 [==============================] - 0s 383us/sample - loss: 0.1074 - accuracy: 0.9944\nEpoch 351/500\n179/179 [==============================] - 0s 379us/sample - loss: 0.1066 - accuracy: 0.9944\nEpoch 352/500\n179/179 [==============================] - 0s 381us/sample - loss: 0.1066 - accuracy: 0.9888\nEpoch 353/500\n179/179 [==============================] - 0s 379us/sample - loss: 0.1052 - accuracy: 0.9944\nEpoch 354/500\n179/179 [==============================] - 0s 384us/sample - loss: 0.1045 - accuracy: 0.9944\nEpoch 355/500\n179/179 [==============================] - 0s 383us/sample - loss: 0.1039 - accuracy: 0.9944\nEpoch 356/500\n179/179 [==============================] - 0s 385us/sample - loss: 0.1031 - accuracy: 0.9944\nEpoch 357/500\n179/179 [==============================] - 0s 376us/sample - loss: 0.1020 - accuracy: 0.9944\nEpoch 358/500\n179/179 [==============================] - 0s 382us/sample - loss: 0.1019 - accuracy: 0.9944\nEpoch 359/500\n179/179 [==============================] - 0s 394us/sample - loss: 0.1007 - accuracy: 0.9944\nEpoch 360/500\n179/179 [==============================] - 0s 376us/sample - loss: 0.1008 - accuracy: 0.9944\nEpoch 361/500\n179/179 [==============================] - 0s 390us/sample - loss: 0.0995 - accuracy: 0.9944\nEpoch 362/500\n179/179 [==============================] - 0s 391us/sample - loss: 0.0998 - accuracy: 0.9944\nEpoch 363/500\n179/179 [==============================] - 0s 383us/sample - loss: 0.0988 - accuracy: 0.9944\nEpoch 364/500\n179/179 [==============================] - 0s 379us/sample - loss: 0.0974 - accuracy: 0.9944\nEpoch 365/500\n179/179 [==============================] - 0s 389us/sample - loss: 0.0969 - accuracy: 0.9944\nEpoch 366/500\n179/179 [==============================] - 0s 378us/sample - loss: 0.0962 - accuracy: 0.9944\nEpoch 367/500\n179/179 [==============================] - 0s 379us/sample - loss: 0.0956 - accuracy: 0.9944\nEpoch 368/500\n179/179 [==============================] - 0s 392us/sample - loss: 0.0949 - accuracy: 0.9944\nEpoch 369/500\n179/179 [==============================] - 0s 376us/sample - loss: 0.0947 - accuracy: 0.9944\nEpoch 370/500\n179/179 [==============================] - 0s 378us/sample - loss: 0.0940 - accuracy: 0.9944\nEpoch 371/500\n179/179 [==============================] - 0s 385us/sample - loss: 0.0936 - accuracy: 0.9944\nEpoch 372/500\n179/179 [==============================] - 0s 398us/sample - loss: 0.0936 - accuracy: 0.9944\nEpoch 373/500\n179/179 [==============================] - 0s 399us/sample - loss: 0.0929 - accuracy: 0.9944\nEpoch 374/500\n179/179 [==============================] - 0s 397us/sample - loss: 0.0925 - accuracy: 0.9944\nEpoch 375/500\n179/179 [==============================] - 0s 387us/sample - loss: 0.0909 - accuracy: 0.9944\nEpoch 376/500\n179/179 [==============================] - 0s 398us/sample - loss: 0.0907 - accuracy: 0.9944\nEpoch 377/500\n179/179 [==============================] - 0s 368us/sample - loss: 0.0904 - accuracy: 0.9888\nEpoch 378/500\n179/179 [==============================] - 0s 373us/sample - loss: 0.0912 - accuracy: 0.9944\nEpoch 379/500\n179/179 [==============================] - 0s 375us/sample - loss: 0.0909 - accuracy: 0.9888\nEpoch 380/500\n179/179 [==============================] - 0s 388us/sample - loss: 0.0890 - accuracy: 0.9944\nEpoch 381/500\n179/179 [==============================] - 0s 387us/sample - loss: 0.0890 - accuracy: 0.9944\nEpoch 382/500\n179/179 [==============================] - 0s 381us/sample - loss: 0.0874 - accuracy: 0.9944\nEpoch 383/500\n179/179 [==============================] - 0s 388us/sample - loss: 0.0864 - accuracy: 0.9944\nEpoch 384/500\n179/179 [==============================] - 0s 396us/sample - loss: 0.0860 - accuracy: 0.9944\nEpoch 385/500\n179/179 [==============================] - 0s 389us/sample - loss: 0.0852 - accuracy: 0.9944\nEpoch 386/500\n179/179 [==============================] - 0s 379us/sample - loss: 0.0845 - accuracy: 0.9944\nEpoch 387/500\n179/179 [==============================] - 0s 384us/sample - loss: 0.0838 - accuracy: 0.9944\nEpoch 388/500\n179/179 [==============================] - 0s 376us/sample - loss: 0.0834 - accuracy: 0.9944\nEpoch 389/500\n179/179 [==============================] - 0s 394us/sample - loss: 0.0832 - accuracy: 0.9944\nEpoch 390/500\n179/179 [==============================] - 0s 421us/sample - loss: 0.0824 - accuracy: 0.9944\nEpoch 391/500\n179/179 [==============================] - 0s 376us/sample - loss: 0.0821 - accuracy: 0.9888\nEpoch 392/500\n179/179 [==============================] - 0s 392us/sample - loss: 0.0811 - accuracy: 0.9944\nEpoch 393/500\n179/179 [==============================] - 0s 390us/sample - loss: 0.0807 - accuracy: 0.9944\nEpoch 394/500\n179/179 [==============================] - 0s 390us/sample - loss: 0.0804 - accuracy: 0.9944\nEpoch 395/500\n179/179 [==============================] - 0s 388us/sample - loss: 0.0804 - accuracy: 0.9944\nEpoch 396/500\n179/179 [==============================] - 0s 404us/sample - loss: 0.0797 - accuracy: 0.9944\nEpoch 397/500\n179/179 [==============================] - 0s 386us/sample - loss: 0.0789 - accuracy: 0.9944\nEpoch 398/500\n179/179 [==============================] - 0s 394us/sample - loss: 0.0780 - accuracy: 0.9944\nEpoch 399/500\n179/179 [==============================] - 0s 386us/sample - loss: 0.0784 - accuracy: 0.9944\nEpoch 400/500\n179/179 [==============================] - 0s 418us/sample - loss: 0.0780 - accuracy: 0.9944\nEpoch 401/500\n179/179 [==============================] - 0s 449us/sample - loss: 0.0767 - accuracy: 0.9944\nEpoch 402/500\n179/179 [==============================] - 0s 404us/sample - loss: 0.0763 - accuracy: 0.9944\nEpoch 403/500\n179/179 [==============================] - 0s 379us/sample - loss: 0.0761 - accuracy: 0.9944\nEpoch 404/500\n179/179 [==============================] - 0s 405us/sample - loss: 0.0757 - accuracy: 0.9944\nEpoch 405/500\n179/179 [==============================] - 0s 406us/sample - loss: 0.0751 - accuracy: 0.9944\nEpoch 406/500\n179/179 [==============================] - 0s 390us/sample - loss: 0.0751 - accuracy: 0.9944\nEpoch 407/500\n179/179 [==============================] - 0s 412us/sample - loss: 0.0747 - accuracy: 0.9944\nEpoch 408/500\n179/179 [==============================] - 0s 392us/sample - loss: 0.0750 - accuracy: 0.9944\nEpoch 409/500\n179/179 [==============================] - 0s 390us/sample - loss: 0.0747 - accuracy: 0.9888\nEpoch 410/500\n179/179 [==============================] - 0s 404us/sample - loss: 0.0738 - accuracy: 0.9944\nEpoch 411/500\n179/179 [==============================] - 0s 401us/sample - loss: 0.0731 - accuracy: 0.9944\nEpoch 412/500\n179/179 [==============================] - 0s 402us/sample - loss: 0.0728 - accuracy: 0.9944\nEpoch 413/500\n179/179 [==============================] - 0s 402us/sample - loss: 0.0715 - accuracy: 0.9944\nEpoch 414/500\n179/179 [==============================] - 0s 384us/sample - loss: 0.0711 - accuracy: 0.9944\nEpoch 415/500\n179/179 [==============================] - 0s 384us/sample - loss: 0.0706 - accuracy: 0.9944\nEpoch 416/500\n179/179 [==============================] - 0s 379us/sample - loss: 0.0700 - accuracy: 0.9944\nEpoch 417/500\n179/179 [==============================] - 0s 400us/sample - loss: 0.0698 - accuracy: 0.9944\nEpoch 418/500\n179/179 [==============================] - 0s 399us/sample - loss: 0.0696 - accuracy: 0.9944\nEpoch 419/500\n179/179 [==============================] - 0s 399us/sample - loss: 0.0688 - accuracy: 0.9888\nEpoch 420/500\n179/179 [==============================] - 0s 400us/sample - loss: 0.0687 - accuracy: 0.9944\nEpoch 421/500\n179/179 [==============================] - 0s 395us/sample - loss: 0.0682 - accuracy: 0.9944\nEpoch 422/500\n179/179 [==============================] - 0s 426us/sample - loss: 0.0682 - accuracy: 0.9888\nEpoch 423/500\n179/179 [==============================] - 0s 383us/sample - loss: 0.0673 - accuracy: 0.9944\nEpoch 424/500\n179/179 [==============================] - 0s 417us/sample - loss: 0.0671 - accuracy: 0.9944\nEpoch 425/500\n179/179 [==============================] - 0s 408us/sample - loss: 0.0665 - accuracy: 0.9944\nEpoch 426/500\n179/179 [==============================] - 0s 400us/sample - loss: 0.0661 - accuracy: 0.9888\nEpoch 427/500\n179/179 [==============================] - 0s 376us/sample - loss: 0.0657 - accuracy: 0.9944\nEpoch 428/500\n179/179 [==============================] - 0s 380us/sample - loss: 0.0655 - accuracy: 0.9944\nEpoch 429/500\n179/179 [==============================] - 0s 396us/sample - loss: 0.0649 - accuracy: 0.9944\nEpoch 430/500\n179/179 [==============================] - 0s 381us/sample - loss: 0.0647 - accuracy: 0.9944\nEpoch 431/500\n179/179 [==============================] - 0s 574us/sample - loss: 0.0644 - accuracy: 0.9944\nEpoch 432/500\n179/179 [==============================] - 0s 384us/sample - loss: 0.0642 - accuracy: 0.9944\nEpoch 433/500\n179/179 [==============================] - 0s 386us/sample - loss: 0.0638 - accuracy: 0.9944\nEpoch 434/500\n179/179 [==============================] - 0s 404us/sample - loss: 0.0632 - accuracy: 0.9944\nEpoch 435/500\n179/179 [==============================] - 0s 413us/sample - loss: 0.0629 - accuracy: 0.9944\nEpoch 436/500\n179/179 [==============================] - 0s 411us/sample - loss: 0.0628 - accuracy: 0.9944\nEpoch 437/500\n179/179 [==============================] - 0s 388us/sample - loss: 0.0626 - accuracy: 0.9944\nEpoch 438/500\n179/179 [==============================] - 0s 383us/sample - loss: 0.0621 - accuracy: 0.9888\nEpoch 439/500\n179/179 [==============================] - 0s 402us/sample - loss: 0.0620 - accuracy: 0.9944\nEpoch 440/500\n179/179 [==============================] - 0s 407us/sample - loss: 0.0613 - accuracy: 0.9944\nEpoch 441/500\n179/179 [==============================] - 0s 582us/sample - loss: 0.0608 - accuracy: 0.9944\nEpoch 442/500\n179/179 [==============================] - 0s 404us/sample - loss: 0.0603 - accuracy: 0.9944\nEpoch 443/500\n179/179 [==============================] - 0s 404us/sample - loss: 0.0603 - accuracy: 0.9888\nEpoch 444/500\n179/179 [==============================] - 0s 406us/sample - loss: 0.0602 - accuracy: 0.9888\nEpoch 445/500\n179/179 [==============================] - 0s 383us/sample - loss: 0.0597 - accuracy: 0.9888\nEpoch 446/500\n179/179 [==============================] - 0s 409us/sample - loss: 0.0592 - accuracy: 0.9944\nEpoch 447/500\n179/179 [==============================] - 0s 406us/sample - loss: 0.0589 - accuracy: 0.9944\nEpoch 448/500\n179/179 [==============================] - 0s 418us/sample - loss: 0.0588 - accuracy: 0.9944\nEpoch 449/500\n179/179 [==============================] - 0s 404us/sample - loss: 0.0585 - accuracy: 0.9944\nEpoch 450/500\n179/179 [==============================] - 0s 420us/sample - loss: 0.0580 - accuracy: 0.9944\nEpoch 451/500\n179/179 [==============================] - 0s 394us/sample - loss: 0.0580 - accuracy: 0.9944\nEpoch 452/500\n179/179 [==============================] - 0s 399us/sample - loss: 0.0575 - accuracy: 0.9888\nEpoch 453/500\n179/179 [==============================] - 0s 401us/sample - loss: 0.0573 - accuracy: 0.9888\nEpoch 454/500\n179/179 [==============================] - 0s 409us/sample - loss: 0.0569 - accuracy: 0.9944\nEpoch 455/500\n179/179 [==============================] - 0s 399us/sample - loss: 0.0565 - accuracy: 0.9944\nEpoch 456/500\n179/179 [==============================] - 0s 391us/sample - loss: 0.0562 - accuracy: 0.9944\nEpoch 457/500\n179/179 [==============================] - 0s 420us/sample - loss: 0.0562 - accuracy: 0.9944\nEpoch 458/500\n179/179 [==============================] - 0s 409us/sample - loss: 0.0561 - accuracy: 0.9888\nEpoch 459/500\n179/179 [==============================] - 0s 399us/sample - loss: 0.0554 - accuracy: 0.9944\nEpoch 460/500\n179/179 [==============================] - 0s 383us/sample - loss: 0.0550 - accuracy: 0.9944\nEpoch 461/500\n179/179 [==============================] - 0s 391us/sample - loss: 0.0545 - accuracy: 0.9944\nEpoch 462/500\n179/179 [==============================] - 0s 395us/sample - loss: 0.0549 - accuracy: 0.9888\nEpoch 463/500\n179/179 [==============================] - 0s 406us/sample - loss: 0.0542 - accuracy: 0.9944\nEpoch 464/500\n179/179 [==============================] - 0s 409us/sample - loss: 0.0539 - accuracy: 0.9944\nEpoch 465/500\n179/179 [==============================] - 0s 393us/sample - loss: 0.0535 - accuracy: 0.9944\nEpoch 466/500\n179/179 [==============================] - 0s 399us/sample - loss: 0.0538 - accuracy: 0.9944\nEpoch 467/500\n179/179 [==============================] - 0s 381us/sample - loss: 0.0531 - accuracy: 0.9944\nEpoch 468/500\n179/179 [==============================] - 0s 374us/sample - loss: 0.0528 - accuracy: 0.9944\nEpoch 469/500\n179/179 [==============================] - 0s 382us/sample - loss: 0.0527 - accuracy: 0.9944\nEpoch 470/500\n179/179 [==============================] - 0s 377us/sample - loss: 0.0526 - accuracy: 0.9944\nEpoch 471/500\n179/179 [==============================] - 0s 371us/sample - loss: 0.0523 - accuracy: 0.9944\nEpoch 472/500\n179/179 [==============================] - 0s 383us/sample - loss: 0.0522 - accuracy: 0.9944\nEpoch 473/500\n179/179 [==============================] - 0s 388us/sample - loss: 0.0522 - accuracy: 0.9944\nEpoch 474/500\n179/179 [==============================] - 0s 386us/sample - loss: 0.0519 - accuracy: 0.9944\nEpoch 475/500\n179/179 [==============================] - 0s 412us/sample - loss: 0.0516 - accuracy: 0.9944\nEpoch 476/500\n179/179 [==============================] - 0s 415us/sample - loss: 0.0517 - accuracy: 0.9944\nEpoch 477/500\n179/179 [==============================] - 0s 401us/sample - loss: 0.0522 - accuracy: 0.9944\nEpoch 478/500\n179/179 [==============================] - 0s 391us/sample - loss: 0.0520 - accuracy: 0.9888\nEpoch 479/500\n179/179 [==============================] - 0s 400us/sample - loss: 0.0626 - accuracy: 0.9888\nEpoch 480/500\n179/179 [==============================] - 0s 380us/sample - loss: 0.2385 - accuracy: 0.9218\nEpoch 481/500\n179/179 [==============================] - 0s 380us/sample - loss: 0.2171 - accuracy: 0.9609\nEpoch 482/500\n179/179 [==============================] - 0s 387us/sample - loss: 0.3187 - accuracy: 0.9218\nEpoch 483/500\n179/179 [==============================] - 0s 385us/sample - loss: 0.2818 - accuracy: 0.9330\nEpoch 484/500\n179/179 [==============================] - 0s 374us/sample - loss: 0.2199 - accuracy: 0.9553\nEpoch 485/500\n179/179 [==============================] - 0s 421us/sample - loss: 0.2039 - accuracy: 0.9665\nEpoch 486/500\n179/179 [==============================] - 0s 400us/sample - loss: 0.2771 - accuracy: 0.9497\nEpoch 487/500\n179/179 [==============================] - 0s 382us/sample - loss: 0.2824 - accuracy: 0.9274\nEpoch 488/500\n179/179 [==============================] - 0s 377us/sample - loss: 0.2992 - accuracy: 0.9385\nEpoch 489/500\n179/179 [==============================] - 0s 395us/sample - loss: 0.3325 - accuracy: 0.8883\nEpoch 490/500\n179/179 [==============================] - 0s 433us/sample - loss: 0.4014 - accuracy: 0.8771\nEpoch 491/500\n179/179 [==============================] - 0s 410us/sample - loss: 0.3243 - accuracy: 0.9106\nEpoch 492/500\n179/179 [==============================] - 0s 376us/sample - loss: 0.4286 - accuracy: 0.8827\nEpoch 493/500\n179/179 [==============================] - 0s 374us/sample - loss: 0.2778 - accuracy: 0.9441\nEpoch 494/500\n179/179 [==============================] - 0s 391us/sample - loss: 0.2596 - accuracy: 0.9553\nEpoch 495/500\n179/179 [==============================] - 0s 380us/sample - loss: 0.2050 - accuracy: 0.9665\nEpoch 496/500\n179/179 [==============================] - 0s 382us/sample - loss: 0.1496 - accuracy: 0.9832\nEpoch 497/500\n179/179 [==============================] - 0s 381us/sample - loss: 0.1280 - accuracy: 0.9888\nEpoch 498/500\n179/179 [==============================] - 0s 405us/sample - loss: 0.1171 - accuracy: 0.9832\nEpoch 499/500\n179/179 [==============================] - 0s 397us/sample - loss: 0.1063 - accuracy: 0.9888\nEpoch 500/500\n179/179 [==============================] - 0s 394us/sample - loss: 0.1013 - accuracy: 0.9888\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x24b3237c8c8>"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "# verbose=1 adds time per sample: 527us/sample \n",
    "h = model.fit(x, y, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24, 21, 73]])"
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "seed = t.texts_to_sequences(['that are expected'])[0]\n",
    "pad_sequences([seed], maxlen=max_sequence_length-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict vs model.predict_classes\n",
    "\n",
    "# gives np array with probabilities of every class\n",
    "predict = model.predict(pad_sequences([seed], maxlen=max_sequence_length-1))\n",
    "\n",
    "# gives number of class with largest prbability\n",
    "predict_classes = model.predict_classes(pad_sequences([seed], maxlen=max_sequence_length-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([4.9465975e-06, 3.4279278e-06, 9.3148118e-03, 2.4137908e-01,\n       8.5217007e-02, 7.7622593e-05, 1.3152923e-02, 1.1193399e-04,\n       6.4593245e-04, 5.9142977e-04], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "# first 10 class probabilities\n",
    "predict[0, 0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.46412545"
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "# max probability value\n",
    "max(predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "92    0.464125\ndtype: float32"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "# number of element with this value\n",
    "import pandas as pd\n",
    "a = pd.Series(predict[0])\n",
    "a[a == max(predict[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text\n",
    "1. Take seed\n",
    "2. Get next word (class with largest prbobability)  \n",
    "Note! Can be modified to take one out of  top 3 prbobabilities!\n",
    "3. Append it to seed\n",
    "4. Repeat with new seed until necessary predicted sequence length is achieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to predict one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Things that are expected running'"
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "def predict_next(seed):\n",
    "    seed_seq = t.texts_to_sequences([seed])[0]\n",
    "    padded_seed = pad_sequences([seed_seq], maxlen=max_sequence_length-1)\n",
    "    word_nr = model.predict_classes(padded_seed, verbose=0)\n",
    "    for word,index in word_index.items():\n",
    "        if word_nr == index:\n",
    "            output_word = word\n",
    "            break\n",
    "    return seed + ' ' + output_word\n",
    "predict_next('Things that are expected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting 10 words from specific seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Things that are expected running a experiment identifying a research small small advising as\n"
    }
   ],
   "source": [
    "seed = 'Things that are expected'\n",
    "for _ in range(10):\n",
    "    seed = predict_next(seed)\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Student research specifies developing a fresh implementation the advisor specifies each management which\n"
    }
   ],
   "source": [
    "seed = 'Student research specifies'\n",
    "for _ in range(10):\n",
    "    seed = predict_next(seed)\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bita866c8c9cf084ebd97979ec2adc3c734",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}